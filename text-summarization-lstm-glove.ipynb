{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":32267,"sourceType":"datasetVersion","datasetId":24984},{"sourceId":791838,"sourceType":"datasetVersion","datasetId":1895}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio","metadata":{"_uuid":"06061c0f-1c0e-402c-871b-434a55454741","_cell_guid":"67a6777e-6bb3-4f3e-be1a-05fbf36a5574","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:29:48.504959Z","iopub.execute_input":"2024-02-02T11:29:48.505720Z","iopub.status.idle":"2024-02-02T11:30:01.450417Z","shell.execute_reply.started":"2024-02-02T11:29:48.505688Z","shell.execute_reply":"2024-02-02T11:30:01.449333Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchtext.vocab import GloVe\nfrom spacy.tokenizer import Tokenizer\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom nltk.corpus import stopwords \nimport random\nfrom tqdm import tqdm","metadata":{"_uuid":"6f86844d-76ea-40a3-971c-d1912cf1586a","_cell_guid":"8bddd0eb-a890-4340-a3c9-0ef8738a2f0c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:01.452928Z","iopub.execute_input":"2024-02-02T11:30:01.453303Z","iopub.status.idle":"2024-02-02T11:30:08.998192Z","shell.execute_reply.started":"2024-02-02T11:30:01.453268Z","shell.execute_reply":"2024-02-02T11:30:08.997388Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenizer using spacy\nnlp = spacy.load(\"en_core_web_sm\")\ntokenizer = Tokenizer(nlp.vocab)","metadata":{"_uuid":"d6bcd342-402a-4535-bb1f-c59b412bd0a5","_cell_guid":"f1f79d85-0899-49e6-a9d7-4039dfcaf2c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:08.999465Z","iopub.execute_input":"2024-02-02T11:30:08.999988Z","iopub.status.idle":"2024-02-02T11:30:10.180847Z","shell.execute_reply.started":"2024-02-02T11:30:08.999963Z","shell.execute_reply":"2024-02-02T11:30:10.179897Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"raw","source":"# Add data from files into dataframe for easier access\ndef create_dataframe(source_text_path,target_text_path):\n    txt_files_source = [file for file in os.listdir(source_text_path) if file.endswith('.txt')]\n    txt_files_target = [file for file in os.listdir(target_text_path) if file.endswith('.txt')]\n    df = pd.DataFrame(columns=['headlines','text'])\n    for source,target in zip(txt_files_source,txt_files_target):\n        assert source==target\n        source_file_path = os.path.join(source_text_path, source)\n        target_file_path = os.path.join(target_text_path, target)\n        # Read the content of the file\n        with open(source_file_path,'r',encoding='latin-1') as file:\n            source_text = file.read()\n        with open(target_file_path,'r',encoding='latin-1') as file:\n            target_text = file.read()\n        df.loc[len(df.index)] = [source_text,target_text]\n    return df","metadata":{"_uuid":"1b492958-5b6f-4884-bace-d44f0fd2b695","_cell_guid":"6bb9f33d-b742-4520-9f77-36475c189436","execution":{"iopub.status.busy":"2024-02-02T04:21:33.492478Z","iopub.execute_input":"2024-02-02T04:21:33.492911Z","iopub.status.idle":"2024-02-02T04:21:33.503243Z","shell.execute_reply.started":"2024-02-02T04:21:33.492874Z","shell.execute_reply":"2024-02-02T04:21:33.502075Z"},"trusted":true}},{"cell_type":"code","source":"# Add data from files into dataframe for easier access\ndef create_dataframe(source_text_path,target_text_path):\n    txt_files_source = [file for file in os.listdir(source_text_path) if file.endswith('.txt')]\n    txt_files_target = [file for file in os.listdir(target_text_path) if file.endswith('.txt')]\n    df = pd.DataFrame(columns=['headlines','text'])\n    for source,target in zip(txt_files_source,txt_files_target):\n        assert source==target\n        source_file_path = os.path.join(source_text_path, source)\n        target_file_path = os.path.join(target_text_path, target)\n        # Read the content of the file\n        with open(source_file_path,'r',encoding='latin-1') as file:\n            source_text = file.read()\n        with open(target_file_path,'r',encoding='latin-1') as file:\n            target_text = file.read()\n        df.loc[len(df.index)] = [source_text,target_text]\n    return df","metadata":{"_uuid":"0d547f7a-e94a-430b-bfa2-f7e14db843cd","_cell_guid":"f8d28a3f-6a14-414d-8a56-32220f8fc5cd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:10.182208Z","iopub.execute_input":"2024-02-02T11:30:10.182940Z","iopub.status.idle":"2024-02-02T11:30:10.190783Z","shell.execute_reply.started":"2024-02-02T11:30:10.182904Z","shell.execute_reply":"2024-02-02T11:30:10.189916Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Check accuracy function\ndef check_accuracy(output,labels):\n    _ , predpos = output.max(1)\n    num_samples=len(labels)\n    num_correct=(predpos==labels).sum()\n    return (num_correct/num_samples)*100\n\n# Save checkpoint\ndef save_checkpoint(state,filename='weights.pth.tar'):\n    print('Saving weights-->')\n    torch.save(state,filename)\n\n# Load checkpoint\ndef load_checkpoint(checkpoint,model,optim):\n    print('Loading weights-->')\n    model.load_state_dict(checkpoint['state_dict'])\n    optim.load_state_dict(checkpoint['optimizer'])","metadata":{"_uuid":"6b87e31f-af18-4979-9876-1cb0da14162b","_cell_guid":"38b19fc0-b1fc-48da-bb22-52092cd2c396","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:10.193709Z","iopub.execute_input":"2024-02-02T11:30:10.194300Z","iopub.status.idle":"2024-02-02T11:30:10.201515Z","shell.execute_reply.started":"2024-02-02T11:30:10.194267Z","shell.execute_reply":"2024-02-02T11:30:10.200755Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df1 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/business\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/business\")\ndf2 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/entertainment\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/entertainment\")\ndf3 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/politics\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/politics\")\ndf4 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/sport\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/sport\")\ndf5 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/tech\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/tech\")","metadata":{"_uuid":"0e1f370d-6318-4a15-8fc8-996c5d3d8910","_cell_guid":"859cb4cf-0dc7-4a42-9bbe-4c97f24ae040","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:10.202557Z","iopub.execute_input":"2024-02-02T11:30:10.202914Z","iopub.status.idle":"2024-02-02T11:30:32.222599Z","shell.execute_reply.started":"2024-02-02T11:30:10.202868Z","shell.execute_reply":"2024-02-02T11:30:32.221794Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)","metadata":{"_uuid":"7b5209f8-bf77-41a2-8c68-f385e646f4d9","_cell_guid":"c394c4de-0042-4a5c-be38-25275eba1067","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:32.223662Z","iopub.execute_input":"2024-02-02T11:30:32.223964Z","iopub.status.idle":"2024-02-02T11:30:32.229332Z","shell.execute_reply.started":"2024-02-02T11:30:32.223939Z","shell.execute_reply":"2024-02-02T11:30:32.228339Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split into train and test sets\ndf = df.rename(columns = {\"headlines\":\"source_text\",\"text\":\"summary_text\"})\nX,Y = df[\"source_text\"],df[\"summary_text\"]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\ntrain_df = pd.DataFrame({'source_text': X_train, 'summary_text': Y_train})\ntest_df = pd.DataFrame({'source_text': X_test, 'summary_text': Y_test})","metadata":{"_uuid":"a5b994d3-e4d9-4d94-b5da-9ae1e5337709","_cell_guid":"c9fe2910-90be-445c-92b2-abe2811884b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:32.230385Z","iopub.execute_input":"2024-02-02T11:30:32.230630Z","iopub.status.idle":"2024-02-02T11:30:32.248484Z","shell.execute_reply.started":"2024-02-02T11:30:32.230592Z","shell.execute_reply":"2024-02-02T11:30:32.247634Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n\n\nstop_words = set(stopwords.words('english'))","metadata":{"_uuid":"aa739b1e-1b35-452c-8db2-18bad8a37ab7","_cell_guid":"99becec1-152d-4333-afa5-adf90637e359","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:32.249500Z","iopub.execute_input":"2024-02-02T11:30:32.249770Z","iopub.status.idle":"2024-02-02T11:30:32.267123Z","shell.execute_reply.started":"2024-02-02T11:30:32.249739Z","shell.execute_reply":"2024-02-02T11:30:32.266293Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def text_cleaner(text):\n    newString = text.lower()\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString)\n    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    tokens = [w for w in newString.split() if not w in stop_words]\n    return \" \".join(tokens)","metadata":{"_uuid":"7af016a3-97f1-478c-89e1-b0ae8642cbe3","_cell_guid":"51839345-ffcc-4379-a3dc-c729827bcba2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:32.268122Z","iopub.execute_input":"2024-02-02T11:30:32.268375Z","iopub.status.idle":"2024-02-02T11:30:32.277470Z","shell.execute_reply.started":"2024-02-02T11:30:32.268353Z","shell.execute_reply":"2024-02-02T11:30:32.276650Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Tokenize and lowercase text using spacy\ntrain_df['source_text'] = train_df['source_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\ntrain_df['summary_text'] = train_df['summary_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\n\ntest_df['source_text'] = test_df['source_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\ntest_df['summary_text'] = test_df['summary_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])","metadata":{"_uuid":"0b5d594b-c083-483e-aa9c-dd87bb1e437c","_cell_guid":"7458418a-e118-47fc-be91-e0641d5f1e65","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:32.278562Z","iopub.execute_input":"2024-02-02T11:30:32.278987Z","iopub.status.idle":"2024-02-02T11:30:36.214677Z","shell.execute_reply.started":"2024-02-02T11:30:32.278955Z","shell.execute_reply":"2024-02-02T11:30:36.213928Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Add START AND END tokens to summary\ntrain_df['source_text'] = train_df['source_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\ntrain_df['summary_text'] = train_df['summary_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\n\ntest_df['source_text'] = test_df['source_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\ntest_df['summary_text'] = test_df['summary_text'].apply(lambda x : ['_START_']+ x + ['_END_'])","metadata":{"_uuid":"5b297723-4ef2-4226-9d90-130a519ccf77","_cell_guid":"2f7360ab-b23e-4000-8cb8-d84442038390","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.215671Z","iopub.execute_input":"2024-02-02T11:30:36.215941Z","iopub.status.idle":"2024-02-02T11:30:36.251229Z","shell.execute_reply.started":"2024-02-02T11:30:36.215918Z","shell.execute_reply":"2024-02-02T11:30:36.250059Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"_uuid":"d3af7c54-06c8-4bb1-87d0-df0a6f732f3f","_cell_guid":"6da2b06f-6afd-4d7f-b248-dfd349297f64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.252518Z","iopub.execute_input":"2024-02-02T11:30:36.252858Z","iopub.status.idle":"2024-02-02T11:30:36.279911Z","shell.execute_reply.started":"2024-02-02T11:30:36.252823Z","shell.execute_reply":"2024-02-02T11:30:36.279085Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            source_text  \\\n1490  [_START_, ferguson, fears, milan, cutting, edg...   \n2001  [_START_, ask, jeeves, joins, web, log, market...   \n1572  [_START_, safin, cool, wimbledon, newly, crown...   \n1840  [_START_, mobiles, rack, years, use, mobile, p...   \n610   [_START_, eminem, secret, gig, venue, revealed...   \n\n                                           summary_text  \n1490  [_START_, loss, could, worse, quality, bring, ...  \n2001  [_START_, jim, lanzone, vice, president, searc...  \n1572  [_START_, expect, sampras, favourite, pressure...  \n1840  [_START_, cellnet, vodafone, mobile, phone, op...  \n610   [_START_, fourth, album, rap, star, sale, two,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_text</th>\n      <th>summary_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1490</th>\n      <td>[_START_, ferguson, fears, milan, cutting, edg...</td>\n      <td>[_START_, loss, could, worse, quality, bring, ...</td>\n    </tr>\n    <tr>\n      <th>2001</th>\n      <td>[_START_, ask, jeeves, joins, web, log, market...</td>\n      <td>[_START_, jim, lanzone, vice, president, searc...</td>\n    </tr>\n    <tr>\n      <th>1572</th>\n      <td>[_START_, safin, cool, wimbledon, newly, crown...</td>\n      <td>[_START_, expect, sampras, favourite, pressure...</td>\n    </tr>\n    <tr>\n      <th>1840</th>\n      <td>[_START_, mobiles, rack, years, use, mobile, p...</td>\n      <td>[_START_, cellnet, vodafone, mobile, phone, op...</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>[_START_, eminem, secret, gig, venue, revealed...</td>\n      <td>[_START_, fourth, album, rap, star, sale, two,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Build vocabularies - each word has an index, note : words sorted in ascending order\nall_tokens = train_df['source_text'].tolist() + train_df['summary_text'].tolist()\nsource_vocab = {actual_word: idx for idx, (word_num, actual_word) in enumerate(sorted(enumerate(set(token for tokens in all_tokens for token in tokens)), key=lambda x: x[1]))}\ntarget_vocab = {actual_word: idx for idx, (word_num, actual_word) in enumerate(sorted(enumerate(set(token for tokens in all_tokens for token in tokens)), key=lambda x: x[1]))}","metadata":{"_uuid":"03d2279f-02c5-4602-967f-4daab5389674","_cell_guid":"2a8d320e-1bab-4d42-8722-4b62182b91df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.284304Z","iopub.execute_input":"2024-02-02T11:30:36.284594Z","iopub.status.idle":"2024-02-02T11:30:36.483677Z","shell.execute_reply.started":"2024-02-02T11:30:36.284570Z","shell.execute_reply":"2024-02-02T11:30:36.482861Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(all_tokens[10])","metadata":{"_uuid":"720949b4-0e4d-4c69-9353-fff9fd6d0926","_cell_guid":"16ba6802-02ac-4ef3-9908-6f75af3efa23","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.484916Z","iopub.execute_input":"2024-02-02T11:30:36.485278Z","iopub.status.idle":"2024-02-02T11:30:36.490741Z","shell.execute_reply.started":"2024-02-02T11:30:36.485244Z","shell.execute_reply":"2024-02-02T11:30:36.489852Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"['_START_', 'watchdog', 'probes', 'e', 'mail', 'deletions', 'information', 'commissioner', 'says', 'urgently', 'asking', 'details', 'cabinet', 'office', 'orders', 'telling', 'staff', 'delete', 'e', 'mails', 'three', 'months', 'old', 'richard', 'thomas', 'totally', 'condemned', 'deletion', 'e', 'mails', 'prevent', 'disclosure', 'freedom', 'information', 'laws', 'coming', 'force', 'january', 'government', 'guidance', 'said', 'e', 'mails', 'deleted', 'served', 'current', 'purpose', 'mr', 'thomas', 'said', 'tories', 'lib', 'dems', 'questioned', 'timing', 'new', 'rules', 'tory', 'leader', 'michael', 'howard', 'written', 'tony', 'blair', 'demanding', 'explanation', 'new', 'rules', 'e', 'mail', 'retention', 'monday', 'lib', 'dem', 'constitutional', 'affairs', 'committee', 'chairman', 'alan', 'beith', 'warned', 'deletion', 'millions', 'government', 'e', 'mails', 'could', 'harm', 'ability', 'key', 'probes', 'like', 'hutton', 'inquiry', 'timing', 'new', 'rules', 'freedom', 'information', 'act', 'comes', 'forces', 'unlikely', 'coincidence', 'mr', 'beith', 'said', 'cabinet', 'office', 'spokeswoman', 'said', 'move', 'new', 'laws', 'destruction', 'important', 'records', 'mr', 'beith', 'urged', 'information', 'commissioner', 'look', 'e', 'mail', 'regime', 'could', 'support', 'freedom', 'information', 'regime', 'mr', 'thomas', 'said', 'new', 'act', 'parliament', 'makes', 'clear', 'destroy', 'records', 'order', 'prevent', 'disclosure', 'becomes', 'criminal', 'offence', 'said', 'already', 'clear', 'guidance', 'retention', 'e', 'mails', 'contained', 'code', 'practice', 'lord', 'chancellor', 'e', 'mails', 'subject', 'freedom', 'information', 'laws', 'important', 'thing', 'content', 'e', 'mail', 'said', 'mr', 'thomas', 'doubt', 'retain', 'long', 'standing', 'principle', 'civil', 'service', 'public', 'authorities', 'got', 'use', 'particular', 'record', 'may', 'legitimate', 'destroy', 'deliberate', 'destruction', 'avoid', 'possibility', 'later', 'disclosure', 'totally', 'condemned', 'freedom', 'information', 'act', 'cover', 'england', 'wales', 'northern', 'ireland', 'next', 'year', 'similar', 'measures', 'brought', 'time', 'scotland', 'provides', 'public', 'right', 'access', 'information', 'held', 'public', 'bodies', 'subject', 'various', 'exemptions', 'implementation', 'monitored', 'information', 'commissioner', '_END_']\n","output_type":"stream"}]},{"cell_type":"code","source":"len(source_vocab)","metadata":{"_uuid":"d23bbf26-0a35-4888-bc01-c287f8bc438c","_cell_guid":"9aeb0e93-4e88-4500-b03b-337722ca2525","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.492147Z","iopub.execute_input":"2024-02-02T11:30:36.492483Z","iopub.status.idle":"2024-02-02T11:30:36.500944Z","shell.execute_reply.started":"2024-02-02T11:30:36.492456Z","shell.execute_reply":"2024-02-02T11:30:36.499926Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"25308"},"metadata":{}}]},{"cell_type":"code","source":"source_vocab == target_vocab","metadata":{"_uuid":"4ecc0e4f-5390-4d88-9e9e-f952ce84cb08","_cell_guid":"06290669-8036-4e24-86ba-52d0e155c50a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.502069Z","iopub.execute_input":"2024-02-02T11:30:36.502384Z","iopub.status.idle":"2024-02-02T11:30:36.514204Z","shell.execute_reply.started":"2024-02-02T11:30:36.502346Z","shell.execute_reply":"2024-02-02T11:30:36.513421Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"temp = list(sorted(source_vocab.items()))\nfor word, idx in temp[-5:]:\n    print(word,idx)","metadata":{"_uuid":"30c22b5b-9415-4146-b3a5-26f442dbcb54","_cell_guid":"6b35cf0a-ae80-4138-9aaa-14da4e1afd5f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.515173Z","iopub.execute_input":"2024-02-02T11:30:36.515451Z","iopub.status.idle":"2024-02-02T11:30:36.528317Z","shell.execute_reply.started":"2024-02-02T11:30:36.515428Z","shell.execute_reply":"2024-02-02T11:30:36.527539Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"zuluaga 25303\nzurich 25304\nzutons 25305\nzvonareva 25306\nzvyagintsev 25307\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load pretrained GloVe embeddings\nglobal_vectors = GloVe(name='6B', dim=300)","metadata":{"_uuid":"b2eeef4e-27e2-44a8-a289-be44f5d4f64f","_cell_guid":"46d7bf6e-c3c3-4cbc-b21c-68218b3ba1eb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:30:36.529399Z","iopub.execute_input":"2024-02-02T11:30:36.529652Z","iopub.status.idle":"2024-02-02T11:34:39.560840Z","shell.execute_reply.started":"2024-02-02T11:30:36.529629Z","shell.execute_reply":"2024-02-02T11:34:39.559757Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":".vector_cache/glove.6B.zip: 862MB [02:38, 5.43MB/s]                              \n100%|█████████▉| 399999/400000 [01:07<00:00, 5967.08it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"source_vectors = torch.stack([global_vectors.get_vecs_by_tokens(word) for word, idx in sorted(source_vocab.items(), key=lambda x: x[1])])\nprint(type(source_vectors), source_vectors.shape)","metadata":{"_uuid":"db553394-8102-473d-9d52-dbcaa39c3cc4","_cell_guid":"faad3483-8b3d-4b03-93b5-835d889e655b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:39.562162Z","iopub.execute_input":"2024-02-02T11:34:39.562472Z","iopub.status.idle":"2024-02-02T11:34:40.041953Z","shell.execute_reply.started":"2024-02-02T11:34:39.562446Z","shell.execute_reply":"2024-02-02T11:34:40.041032Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"<class 'torch.Tensor'> torch.Size([25308, 300])\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nThe source_vectors is the predefined word to vector mapping we have created from pretrained Glove Embeddings.\nWe use this as input to the Embedding Layer, which will not be trained from scratch.\nLets say a sentence is passed as input Eg : [658930, 9289283, 2624242, 89798, 53424]\nThe Embedding layer performs a lookup operation for every word in sentence using the source_vectors. \nand this input of size [1,5] gets converted to [1,5,100] \n'''","metadata":{"_uuid":"52d51420-0f52-407e-8a06-00131dfa4f03","_cell_guid":"4a9144fc-310d-47b0-bca5-20624d055ed0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.043135Z","iopub.execute_input":"2024-02-02T11:34:40.043416Z","iopub.status.idle":"2024-02-02T11:34:40.049638Z","shell.execute_reply.started":"2024-02-02T11:34:40.043390Z","shell.execute_reply":"2024-02-02T11:34:40.048786Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'\\nThe source_vectors is the predefined word to vector mapping we have created from pretrained Glove Embeddings.\\nWe use this as input to the Embedding Layer, which will not be trained from scratch.\\nLets say a sentence is passed as input Eg : [658930, 9289283, 2624242, 89798, 53424]\\nThe Embedding layer performs a lookup operation for every word in sentence using the source_vectors. \\nand this input of size [1,5] gets converted to [1,5,100] \\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"a88a0823-a239-41a0-9377-e8f8883012af","_cell_guid":"c21a0199-3ed8-4d9c-808e-0dfa2204e8f6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.050773Z","iopub.execute_input":"2024-02-02T11:34:40.051109Z","iopub.status.idle":"2024-02-02T11:34:40.063993Z","shell.execute_reply.started":"2024-02-02T11:34:40.051085Z","shell.execute_reply":"2024-02-02T11:34:40.062994Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Define a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, source_texts, target_summaries, source_vocab, target_vocab):\n        self.source_texts = source_texts\n        self.target_summaries = target_summaries\n        self.source_vocab = source_vocab\n        self.target_vocab = target_vocab\n\n    def __len__(self):\n        return len(self.source_texts)\n\n    def __getitem__(self, idx):\n        source_text = [self.source_vocab[word] for word in self.source_texts[idx]]\n        target_summary = [self.target_vocab[word] for word in self.target_summaries[idx]]\n        return torch.tensor(source_text), torch.tensor(target_summary)","metadata":{"_uuid":"5c700ef7-1d9c-431a-90f4-69541b6d447f","_cell_guid":"97ff7f25-d2e1-48a1-a766-9ec99af6b06c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.065119Z","iopub.execute_input":"2024-02-02T11:34:40.065370Z","iopub.status.idle":"2024-02-02T11:34:40.074669Z","shell.execute_reply.started":"2024-02-02T11:34:40.065348Z","shell.execute_reply":"2024-02-02T11:34:40.073911Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Create custom datasets\ntrain_dataset = CustomDataset(train_df['source_text'].tolist(), train_df['summary_text'].tolist(), source_vocab, target_vocab)\ntest_dataset = CustomDataset(test_df['source_text'].tolist(), test_df['summary_text'].tolist(), source_vocab, target_vocab)","metadata":{"_uuid":"b9c2e425-00ab-4835-a2cb-25cb4e9e5351","_cell_guid":"95a101e9-fcad-4b25-96a2-855995a21625","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.075608Z","iopub.execute_input":"2024-02-02T11:34:40.075922Z","iopub.status.idle":"2024-02-02T11:34:40.084447Z","shell.execute_reply.started":"2024-02-02T11:34:40.075870Z","shell.execute_reply":"2024-02-02T11:34:40.083684Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"'''\nNote : \nIn PyTorch, the `collate_fn` parameter in the `DataLoader` can be either a function or an object of a class. Both approaches are valid, and the choice depends on your preference and the complexity of your collation logic.\n\n1. Function as `collate_fn`:\ndef my_collate_fn(batch):\n    # Your custom collation logic here\n    return processed_batch\n# Use the function with DataLoader\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_fn)\n\n2. Class as `collate_fn`:\nclass MyCollateClass:\n    def __call__(self, batch):\n        # Your custom collation logic here\n        return processed_batch\n# Instantiate the class and use it with DataLoader\nmy_collate_instance = MyCollateClass()\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_instance)\n\nUsing a class allows you to maintain state between batches if needed, as the class instance retains its state between calls. This can be beneficial if your collation logic requires some persistent information.\n\nThe key point is that the `collate_fn` parameter should be a callable (a function or an object with a `__call__` method) that takes a list of batch data and returns the processed batch. The processing typically involves padding sequences, converting data types, or any other necessary steps to prepare the batch for the model.\n'''","metadata":{"_uuid":"cb26b53b-19be-4322-a385-1edceb08cdc2","_cell_guid":"bff0c7b0-98d6-4f58-b23e-5cebcea4f475","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.085392Z","iopub.execute_input":"2024-02-02T11:34:40.085651Z","iopub.status.idle":"2024-02-02T11:34:40.095606Z","shell.execute_reply.started":"2024-02-02T11:34:40.085628Z","shell.execute_reply":"2024-02-02T11:34:40.094689Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'\\nNote : \\nIn PyTorch, the `collate_fn` parameter in the `DataLoader` can be either a function or an object of a class. Both approaches are valid, and the choice depends on your preference and the complexity of your collation logic.\\n\\n1. Function as `collate_fn`:\\ndef my_collate_fn(batch):\\n    # Your custom collation logic here\\n    return processed_batch\\n# Use the function with DataLoader\\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_fn)\\n\\n2. Class as `collate_fn`:\\nclass MyCollateClass:\\n    def __call__(self, batch):\\n        # Your custom collation logic here\\n        return processed_batch\\n# Instantiate the class and use it with DataLoader\\nmy_collate_instance = MyCollateClass()\\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_instance)\\n\\nUsing a class allows you to maintain state between batches if needed, as the class instance retains its state between calls. This can be beneficial if your collation logic requires some persistent information.\\n\\nThe key point is that the `collate_fn` parameter should be a callable (a function or an object with a `__call__` method) that takes a list of batch data and returns the processed batch. The processing typically involves padding sequences, converting data types, or any other necessary steps to prepare the batch for the model.\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Define collate function for DataLoader\ndef collate_fn(batch):\n    sources, targets = zip(*batch)\n    padded_sources = pad_sequence(sources, batch_first=True)\n    padded_targets = pad_sequence(targets, batch_first=True)\n    return padded_sources, padded_targets","metadata":{"_uuid":"c0ae1f56-d0fd-45ae-a089-b641b87f4300","_cell_guid":"0c044cfc-0ffa-4c2c-a9b3-33714aecf16b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.096872Z","iopub.execute_input":"2024-02-02T11:34:40.097472Z","iopub.status.idle":"2024-02-02T11:34:40.108197Z","shell.execute_reply.started":"2024-02-02T11:34:40.097439Z","shell.execute_reply":"2024-02-02T11:34:40.107438Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Define the Encoder Architecture using LSTM\nclass Encoder(nn.Module):\n    def __init__(self, source_vectors, embedding_dim, hidden_dim, n_layers, dropout):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(source_vectors, freeze=False)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n\n    def forward(self, X):\n        # X shape = [Batch_Size X Sequence_Len X 1]\n        X = self.embedding(X)\n        # X shape = [Batch_Size X Sequence_Len X Embedding_Dim]\n        assert X.shape[0]>0 and X.shape[1]>0\n        X,(hidden_state,cell_state) = self.lstm(X)\n        # X shape = [Batch_Size X Seq_Len X Hidden_Dim] , Hidden_State_Shape = Cell_State_Shape = [Num_Layers X Batch_Size X Hidden_Dim]\n        return hidden_state,cell_state\n\n# Define the Decoder Architecture using LSTM\nclass Decoder(nn.Module):\n    def __init__(self, source_vectors, target_vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n        super(Decoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.target_vocab_size = target_vocab_size\n        self.embedding = nn.Embedding.from_pretrained(source_vectors, freeze=False)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_dim*2,target_vocab_size) # bidrectional hence \n\n    def forward(self, hidden_state, cell_state, Y, force_correction=0.5):\n        # Hidden_State_Shape = Cell_State_Shape = [Num_Layers X Batch_Size X Hidden_Dim]\n        # Y Shape = [Batch_Size X Sequence_Len]\n        \n        batch_size,seq_len = Y.shape[0],Y.shape[1]\n        outputs = torch.zeros(seq_len,batch_size,self.target_vocab_size,requires_grad=True).to(device) # [Batch_Size X Sequence_Len]\n        \n        X = Y[:,1]\n        # X shape = [Batch_Size X 1]\n        for i in range(seq_len):\n            X = X.unsqueeze(1) \n            # X shape = [Batch_Size X 1 X 1]\n            decoder_input = self.embedding(X)\n            # decoder_input_shape = [Batch_Size X 1 X Embedding_Dim]\n            assert decoder_input.shape[0]>0 and decoder_input.shape[1]>0\n            decoder_output,(hidden_state,cell_state) = self.lstm(decoder_input,(hidden_state,cell_state))\n            # Decoder_Output_Shape = [Batch_Size X 1 X Target_Vocab_Size]\n            decoder_output = self.fc(decoder_output)\n            # Store output\n            outputs[i] = decoder_output.permute(1,0,2).squeeze(0)\n            _ , indexes = decoder_output.max(dim=2)\n            # indexes shape = [Batch_Size X 1]\n            indexes = indexes.squeeze(1)\n            # use indexes as next input or correct it\n            X = indexes if random.random() < 0.5 else Y[:,i]\n            # indexes shape = X shape = [Batch_Size]\n            \n        # Output Shape = [Seq_Len X Batch_Size X Target_Vocab_Size]\n        outputs = outputs.permute(1,0,2)\n        outputs = outputs.reshape(-1,self.target_vocab_size)\n        # Output Shape = [Batch_Size X Seq_Len X Target_Vocab_Size]\n        return outputs","metadata":{"_uuid":"d728e87a-b24b-4714-8b3b-a54d1239e753","_cell_guid":"c37fbbbe-1473-4abe-85db-e70641811ac5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.109445Z","iopub.execute_input":"2024-02-02T11:34:40.109737Z","iopub.status.idle":"2024-02-02T11:34:40.123773Z","shell.execute_reply.started":"2024-02-02T11:34:40.109713Z","shell.execute_reply":"2024-02-02T11:34:40.123106Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class EncDecLSTM(nn.Module):\n    def __init__(self,enc,dec):\n        super(EncDecLSTM,self).__init__()\n        self.enc = enc\n        self.dec = dec\n        \n    def forward(self,X,Y):\n        hidden_state,cell_state = self.enc(X)\n        output = self.dec(hidden_state,cell_state,Y)\n        return output","metadata":{"_uuid":"4cf8aee3-ef6e-4dce-9626-a24b01e27d11","_cell_guid":"5f08063b-4e48-463d-ad9a-6fe9ae4fbebc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.124992Z","iopub.execute_input":"2024-02-02T11:34:40.125369Z","iopub.status.idle":"2024-02-02T11:34:40.136137Z","shell.execute_reply.started":"2024-02-02T11:34:40.125338Z","shell.execute_reply":"2024-02-02T11:34:40.135358Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\ninput_dim = len(source_vocab)\noutput_dim = len(target_vocab)\nlearning_rate = 0.001\nembedding_dim = 300  \nhidden_dim = 512\nn_layers = 2\ndropout = 0.2\nnum_epochs = 25\nnum_workers = 2\n\nencoder = Encoder(source_vectors, embedding_dim, hidden_dim, n_layers, dropout)\ndecoder = Decoder(source_vectors, output_dim, embedding_dim, hidden_dim, n_layers, dropout)\nmodel = EncDecLSTM(encoder,decoder)\nprint(model)","metadata":{"_uuid":"7f8ac48d-4b9e-47cb-ba43-3e70a9a88825","_cell_guid":"562557a5-f391-4d05-aae0-c7c30daf0791","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.137204Z","iopub.execute_input":"2024-02-02T11:34:40.137444Z","iopub.status.idle":"2024-02-02T11:34:40.500575Z","shell.execute_reply.started":"2024-02-02T11:34:40.137422Z","shell.execute_reply":"2024-02-02T11:34:40.499597Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"EncDecLSTM(\n  (enc): Encoder(\n    (embedding): Embedding(25308, 300)\n    (lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (dec): Decoder(\n    (embedding): Embedding(25308, 300)\n    (lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=25308, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\nprint(trainable_params)","metadata":{"_uuid":"a29591bc-e493-4356-8ce4-0bd82edbf295","_cell_guid":"6f1133e6-fe79-4ace-bdcb-1071d488bb46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.501760Z","iopub.execute_input":"2024-02-02T11:34:40.502091Z","iopub.status.idle":"2024-02-02T11:34:40.507466Z","shell.execute_reply.started":"2024-02-02T11:34:40.502064Z","shell.execute_reply":"2024-02-02T11:34:40.506442Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"17226192\n","output_type":"stream"}]},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(trainable_params)","metadata":{"_uuid":"12e66484-30b1-4f7e-936c-ba517528b578","_cell_guid":"d6077383-9efa-470d-8f9e-9f83631293f2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.508536Z","iopub.execute_input":"2024-02-02T11:34:40.508917Z","iopub.status.idle":"2024-02-02T11:34:40.517047Z","shell.execute_reply.started":"2024-02-02T11:34:40.508867Z","shell.execute_reply":"2024-02-02T11:34:40.516206Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"60393084\n","output_type":"stream"}]},{"cell_type":"code","source":"# Specify optimizer and loss function\noptimizer = optim.Adam(model.parameters(),lr=learning_rate)\nloss_fun = nn.CrossEntropyLoss()","metadata":{"_uuid":"729c96c0-58f3-420d-897c-17fb2cea6636","_cell_guid":"096d846a-1fa7-4ff0-b3d0-cf14a4d73fe3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.518085Z","iopub.execute_input":"2024-02-02T11:34:40.518357Z","iopub.status.idle":"2024-02-02T11:34:40.527802Z","shell.execute_reply.started":"2024-02-02T11:34:40.518333Z","shell.execute_reply":"2024-02-02T11:34:40.527084Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)","metadata":{"_uuid":"e1d352b6-e6eb-4f4e-a0e8-73c1c60073e0","_cell_guid":"e61dd8da-a789-42b2-918b-f794b386cae3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.528828Z","iopub.execute_input":"2024-02-02T11:34:40.529126Z","iopub.status.idle":"2024-02-02T11:34:40.542701Z","shell.execute_reply.started":"2024-02-02T11:34:40.529103Z","shell.execute_reply":"2024-02-02T11:34:40.541807Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"source_dummy,target_dummy = next(iter(train_loader))","metadata":{"_uuid":"66926910-b0f7-4f0f-aeba-608377d2fd9e","_cell_guid":"caa85f17-1a9e-44b7-9f3b-8bd685adb4ed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.543649Z","iopub.execute_input":"2024-02-02T11:34:40.543937Z","iopub.status.idle":"2024-02-02T11:34:40.652446Z","shell.execute_reply.started":"2024-02-02T11:34:40.543908Z","shell.execute_reply":"2024-02-02T11:34:40.651430Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(source_dummy.shape,target_dummy.shape)","metadata":{"_uuid":"f3d11ffc-53d7-4b60-a296-b27f2427e771","_cell_guid":"c5058099-46f6-4528-9b22-2209a930018f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.653832Z","iopub.execute_input":"2024-02-02T11:34:40.654177Z","iopub.status.idle":"2024-02-02T11:34:40.659277Z","shell.execute_reply.started":"2024-02-02T11:34:40.654150Z","shell.execute_reply":"2024-02-02T11:34:40.658340Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"torch.Size([8, 324]) torch.Size([8, 143])\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model(source_dummy,target_dummy)\nprint(y_pred.shape,target_dummy.shape)","metadata":{"_uuid":"63ac2abe-395a-474c-84b1-25be78bcd7e0","_cell_guid":"dbc83b3c-e6cb-46d1-b98f-9b96005d388e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:34:40.660322Z","iopub.execute_input":"2024-02-02T11:34:40.660587Z","iopub.status.idle":"2024-02-02T11:34:45.108866Z","shell.execute_reply.started":"2024-02-02T11:34:40.660563Z","shell.execute_reply":"2024-02-02T11:34:45.107869Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"torch.Size([1144, 25308]) torch.Size([8, 143])\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_loop(encoder,decoder,dataloader,loss_fun,optimizer,device):\n    encoder.train()\n    encoder.to(device)\n    decoder.train()\n    decoder.to(device)\n    min_loss = None\n    for epoch in range(num_epochs):\n        losses = []\n        accuracies = []\n        loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=True)\n        for batch,(x,y) in loop:\n            # put on cuda\n            x = x.to(device)\n            y = y.to(device)\n    \n            # forward pass\n            y_pred = model(x,y)\n            \n            # calculate loss & accuracy\n            loss = loss_fun(y_pred,y.reshape(-1))\n            losses.append(loss.detach().item())\n            \n            accuracy = check_accuracy(y_pred,y.reshape(-1))\n            accuracies.append(accuracy.detach().item())\n            \n            # zero out prior gradients\n            optimizer.zero_grad()\n            \n            # backprop\n            loss.backward()\n            \n            # update weights\n            optimizer.step()\n            \n            # Update TQDM progress bar\n            loop.set_description(f\"Epoch [{epoch}/{num_epochs}] \")\n            loop.set_postfix(loss=loss.detach().item(), accuracy=accuracy.detach().item())\n\n        moving_loss = sum(losses) / len(losses)\n        moving_accuracy = sum(accuracies) / len(accuracies)\n        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n        # Save check point\n        if min_loss == None:\n            min_loss = moving_loss\n            save_checkpoint(checkpoint)\n        elif moving_loss < min_loss:\n            min_loss = moving_loss\n            save_checkpoint(checkpoint)\n        print('Epoch {0} : Loss = {1} , Accuracy={2}'.format(epoch, moving_loss, moving_accuracy))","metadata":{"_uuid":"f2ac494c-69a7-4382-97d2-44cb898425fb","_cell_guid":"638d353d-2acf-4729-ba57-814a1f6b3bc4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:41:52.229279Z","iopub.execute_input":"2024-02-02T11:41:52.230047Z","iopub.status.idle":"2024-02-02T11:41:52.241201Z","shell.execute_reply.started":"2024-02-02T11:41:52.230015Z","shell.execute_reply":"2024-02-02T11:41:52.240126Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_loop(encoder,decoder,train_loader,loss_fun,optimizer,device)","metadata":{"_uuid":"3e3ab4e6-c9bc-4aca-8fb2-72b5662c064a","_cell_guid":"062d2181-a1fb-44b3-b2c9-7b16d270e4f8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-02T11:41:52.606227Z","iopub.execute_input":"2024-02-02T11:41:52.606563Z","iopub.status.idle":"2024-02-02T14:12:09.915997Z","shell.execute_reply.started":"2024-02-02T11:41:52.606538Z","shell.execute_reply":"2024-02-02T14:12:09.914209Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Epoch [0/25] : 100%|██████████| 223/223 [05:57<00:00,  1.60s/it, accuracy=36.4, loss=5.32]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 0 : Loss = 4.8169199768203255 , Accuracy=43.79491660627014\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/25] : 100%|██████████| 223/223 [05:53<00:00,  1.59s/it, accuracy=51.7, loss=3.85]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 1 : Loss = 4.610716999378974 , Accuracy=44.13662564914857\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/25] : 100%|██████████| 223/223 [06:00<00:00,  1.62s/it, accuracy=36.5, loss=5.07]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 2 : Loss = 4.443664215070784 , Accuracy=44.40977132480775\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/25] : 100%|██████████| 223/223 [05:59<00:00,  1.61s/it, accuracy=41.8, loss=4.56]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 3 : Loss = 4.279679609521088 , Accuracy=44.82060778835964\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/25] : 100%|██████████| 223/223 [06:00<00:00,  1.62s/it, accuracy=28.5, loss=5.35]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 4 : Loss = 4.144228878577194 , Accuracy=45.10568827470856\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/25] : 100%|██████████| 223/223 [05:57<00:00,  1.60s/it, accuracy=52.8, loss=3.55]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 5 : Loss = 4.0475257624425165 , Accuracy=44.93337908347091\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/25] : 100%|██████████| 223/223 [05:56<00:00,  1.60s/it, accuracy=38.7, loss=4.17]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 6 : Loss = 3.9077368825005845 , Accuracy=45.363040864200336\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/25] : 100%|██████████| 223/223 [05:57<00:00,  1.60s/it, accuracy=49.7, loss=3.56]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 7 : Loss = 3.7733583637417163 , Accuracy=45.759215034176954\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/25] : 100%|██████████| 223/223 [05:59<00:00,  1.61s/it, accuracy=48.1, loss=3.4] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 8 : Loss = 3.5881300282585245 , Accuracy=46.86658919980173\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/25] : 100%|██████████| 223/223 [05:59<00:00,  1.61s/it, accuracy=34.8, loss=4.39]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 9 : Loss = 3.4644860563791386 , Accuracy=47.372889984883535\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/25] : 100%|██████████| 223/223 [05:52<00:00,  1.58s/it, accuracy=25, loss=4.74]  \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 10 : Loss = 3.3623503072379415 , Accuracy=47.52821568202545\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/25] : 100%|██████████| 223/223 [06:01<00:00,  1.62s/it, accuracy=42.4, loss=3.66]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 11 : Loss = 3.1475155075569323 , Accuracy=49.79815477106069\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/25] : 100%|██████████| 223/223 [05:55<00:00,  1.60s/it, accuracy=49.3, loss=3.15]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 12 : Loss = 3.0223503973451966 , Accuracy=50.75443864830941\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/25] : 100%|██████████| 223/223 [06:00<00:00,  1.61s/it, accuracy=33.7, loss=4.03]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 13 : Loss = 2.811319390754529 , Accuracy=53.09127147315329\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/25] : 100%|██████████| 223/223 [05:55<00:00,  1.59s/it, accuracy=50, loss=2.98]  \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 14 : Loss = 2.6896044785666358 , Accuracy=54.140135666714656\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/25] : 100%|██████████| 223/223 [06:01<00:00,  1.62s/it, accuracy=58.9, loss=2.46]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 15 : Loss = 2.528768129947474 , Accuracy=56.080495945541315\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/25] : 100%|██████████| 223/223 [06:00<00:00,  1.62s/it, accuracy=50.3, loss=2.87]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 16 : Loss = 2.382731790499837 , Accuracy=57.724022116896286\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/25] : 100%|██████████| 223/223 [06:02<00:00,  1.63s/it, accuracy=57.2, loss=2.28]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 17 : Loss = 2.216430801447197 , Accuracy=59.89502105370765\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/25] : 100%|██████████| 223/223 [05:54<00:00,  1.59s/it, accuracy=45.3, loss=3.15]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 18 : Loss = 2.148166098936791 , Accuracy=60.54665638192352\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/25] : 100%|██████████| 223/223 [06:01<00:00,  1.62s/it, accuracy=49.4, loss=2.79]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 19 : Loss = 1.973728955059308 , Accuracy=63.227892238462985\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/25] : 100%|██████████| 223/223 [06:02<00:00,  1.63s/it, accuracy=52.7, loss=2.49]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 20 : Loss = 1.863277077140295 , Accuracy=64.87480899143647\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/25] : 100%|██████████| 223/223 [06:01<00:00,  1.62s/it, accuracy=76, loss=1.18]   \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 21 : Loss = 1.742361314360871 , Accuracy=66.6836456161978\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/25] : 100%|██████████| 223/223 [05:56<00:00,  1.60s/it, accuracy=74, loss=1.25]   \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 22 : Loss = 1.6295097064009696 , Accuracy=68.6317094195584\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/25] : 100%|██████████| 223/223 [05:59<00:00,  1.61s/it, accuracy=64.4, loss=1.77] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 23 : Loss = 1.5311317812701513 , Accuracy=70.27389476759016\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/25] : 100%|██████████| 223/223 [06:03<00:00,  1.63s/it, accuracy=71.7, loss=1.45] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 24 : Loss = 1.3879474823784934 , Accuracy=72.81760350043463\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"_uuid":"e0a55155-c1de-4749-b58a-4920626a0e72","_cell_guid":"5509e0a8-6ec0-4fc7-89f1-ffb4d25b17ee","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"bbea0721-a2e9-4816-83cf-cde54fccc62d","_cell_guid":"61da884e-a668-4b82-902f-75d66ea0b414","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}