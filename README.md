# Abstractive-Text-Summarization
<ul>
  <li> Created custom vocabulary and dataset in Pytorch to fetch text samples and their summaries for training and validation data loader
  <li> Compared context-independent GloVe word embeddings and context-dependent BERT word embeddings for abstractive text summarization using Encoder-Decoder Bi-LSTM with 2 layers, using cosine annealing with Adam optimizer. 
  <li> Implemented transformer model from scratch with multi-head attention mechanism and positional encoding for abstractive text summarization from research paper.
  <li> Evaluated and compared accuracy of transformer architecture with self-attention mechanism against Encoder-Decoder with Bi-LSTM.
</ul>
