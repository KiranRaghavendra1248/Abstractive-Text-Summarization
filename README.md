# Abstractive-Text-Summarization
<ul>
  <li> Created custom dataset in Pytorch to fetch text samples and their summaries for training and validation data loader
  <li> Compared context-independent GloVe word embeddings and context-dependent BERT word embeddings for abstractive text summarization using Encoder-Decoder Bi-LSTM with 2 layers, using cosine annealing with Adam optimizer. 
  <li> Implemented transformer model from scratch with multi-head attention mechanism and positional encoding for abstractive text summarization from research paper.
  <li> TBD: Evaluate and compare accuracy of transformer architecture with self-attention mechanism against seq2seq RNNs with LSTM and added attention layer using pre-trained GloVe word embeddings.
</ul>
