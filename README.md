# Abstractive-Text-Summarization
<ul>
  <li> Performed text preprocessing by tokenizing and removing stop words on BBC News Summary Dataset
  <li> Create dataset in Pytorch to fetch text samples and their summaries
  <li> Built vocabulary using tokens in the BBC News dataset and utilized pre-trained GloVe word embeddings for tokens during training
  <li> Implement transformer model from scratch with multi-head attention mechanism and positional encoding for abstractive text summarization.
  <li> TBD: Evaluate and compare accuracy of transformer architecture with self-attention mechanism against seq2seq RNNs with LSTM and added attention layer using pre-trained GloVe word embeddings.
</ul>
