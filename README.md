# Abstractive-Text-Summarization
<ul>
  <li> Performed text preprocessing by tokenizing and removing stop words on BBC News Summary Dataset
  <li> Create dataset in Pytorch to fetch text samples and their summaries
  <li> Built vocabulary using tokens in the BBC News dataset and utilized pre-trained GloVe word embeddings for tokens during training
  <li> TBD: Evaluate and compare the accuracy of Seq2Seq RNNs with LSTM and Attention Layer against Tranformer Architecture with Self Attention Mechanism.
</ul>
